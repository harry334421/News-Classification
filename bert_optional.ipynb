{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install pandas==0.24.0\n!pip install tqdm==4.32.2\n!pip install tqdm boto3 requests regex\n!pip install pytorch_pretrained_bert pytorch-nlp","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"%matplotlib inline\nimport sys\nimport itertools\nimport numpy as np\nimport random as rn\nimport matplotlib.pyplot as plt\nimport torch\nfrom pytorch_pretrained_bert import BertModel\nfrom torch import nn\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.optim import Adam\nfrom torch.nn.utils import clip_grad_norm_\nfrom IPython.display import clear_output\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm\nimport pandas as pd\ntqdm.pandas()\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Load data"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"dataset = pd.read_json('/kaggle/input/News_Category_Dataset_v2.json', lines=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# preprocessing"},{"metadata":{},"cell_type":"markdown","source":"## fix categories"},{"metadata":{"trusted":true},"cell_type":"code","source":"cats = dataset.groupby('category')\nprint(cats.size(), \"\\n total groups: \",cats.ngroups)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge categories which should be the same\ndataset.category.replace('THE WORLDPOST', 'WORLDPOST', inplace=True)\ndataset.category.replace('WORLDPOST', 'WORLD NEWS', inplace=True)\ndataset.category.replace('ARTS', 'ARTS & CULTURE', inplace=True)\ndataset.category.replace('CULTURE & ARTS', 'ARTS & CULTURE', inplace=True)\ndataset.category.replace('PARENTS', 'PARENTING', inplace=True)\ndataset.category.replace('STYLE', 'STYLE & BEAUTY', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## make new contents column"},{"metadata":{"trusted":true},"cell_type":"code","source":"# headline plus short description\ndataset['contents'] = dataset.headline + \". \" + dataset.short_description","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## split the data into train and test"},{"metadata":{"trusted":true},"cell_type":"code","source":"train, test = train_test_split(dataset, test_size=0.2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"rn.shuffle(train)\nrn.shuffle(test)"},{"metadata":{"trusted":true},"cell_type":"code","source":"train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_contents, train_cats = train['contents'] , train['category'] # select the test and training data\ntest_contents, test_cats = test['contents'] , test['category']\nlen(train_contents), len(train_cats), len(test_contents), len(test_cats)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Tokenize"},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True) # initialise the BERT tokenizer","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 128","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tokens = train_contents.progress_apply(lambda x: ['[CLS]'] + tokenizer.tokenize(x)[:MAX_LEN] + ['[SEP]']) # BERT accepts a [CLS] token as the start of a sentence, so for each dataset add this and clip it at max 511 lenth. [SEP] should be at the end?\ntest_tokens = test_contents.progress_apply(lambda x: ['[CLS]'] + tokenizer.tokenize(x)[:MAX_LEN] + ['[SEP]'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tokens.iloc[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tokens_ids = train_tokens.progress_apply(tokenizer.convert_tokens_to_ids) # convert the tokens to ids\ntest_tokens_ids = test_tokens.progress_apply(tokenizer.convert_tokens_to_ids)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tokens_ids","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tokens_ids = pad_sequences(train_tokens_ids, maxlen=MAX_LEN, truncating=\"post\", padding=\"post\", dtype=\"int\") # pad sequences\ntest_tokens_ids = pad_sequences(test_tokens_ids, maxlen=MAX_LEN, truncating=\"post\", padding=\"post\", dtype=\"int\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tokens_ids # they're now numpy arrays","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_tokens_ids.shape, test_tokens_ids.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"## now convert the labels to numbers"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = train_cats.astype('category').cat.codes #  convert each categorical label into a number to label the category\ntest_y = test_cats.astype('category').cat.codes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y = train_y.astype('int64').values\ntest_y = test_y.astype('int64').values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#  now convert these to one hot encoded!\n#no_labels = len(train_cats.astype('category').cat.categories)\n#train_y = np.eye(no_labels)[train_y] # creates onehot array\n#test_y = np.eye(no_labels)[test_y]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_y.shape, test_y.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## masks"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_masks = (train_tokens_ids > 0).astype('float')\ntrain_masks = train_masks.tolist()\n\ntest_masks = (test_tokens_ids > 0).astype('float')\ntest_masks = test_masks.tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_masks","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# run baseline"},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_model = make_pipeline(CountVectorizer(ngram_range=(1,3)), SGDClassifier(loss='log', verbose=2, n_jobs=-1, max_iter=10, learning_rate='optimal', n_iter_no_change=5, validation_fraction=0.1, alpha=0.001)) # use sgd as we have a large model. log loss makes it logistic regression","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_model = baseline_model.fit(train_contents, train_cats)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\npkl_filename = \"../working/news_logistic_bigsgd_higherreg.pkl\"\nwith open(pkl_filename, 'wb') as file:\n    pickle.dump(baseline_model, file)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\npkl_filename = \"../working/news_logistic_bigsgd_higherreg.pkl\"\npickle.load(open(pkl_filename, \"rb\"))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"baseline_predicted = baseline_model.predict(test_contents) # predict the categories for the test set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(test_cats, baseline_predicted)) # report the scores","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# BERT pretrained model"},{"metadata":{},"cell_type":"markdown","source":"## general setup"},{"metadata":{"trusted":true},"cell_type":"code","source":"rn.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## load classifier"},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_pretrained_bert import BertForSequenceClassification","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"no_labels = len(train_cats.astype('category').cat.categories)\nbert_clf = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=no_labels)\nbert_clf = bert_clf.cuda()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load model params"},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 64\nEPOCHS = 4","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"param_optimizer = list(bert_clf.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from pytorch_pretrained_bert import BertAdam\noptimizer = BertAdam(optimizer_grouped_parameters,\n                     lr=3e-5,\n                     warmup=.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## setup data as tensors for pytorch"},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert to tensors\ntrain_tokens_tensor = torch.tensor(train_tokens_ids)\ntrain_y_tensor = torch.tensor(train_y, dtype=torch.long)\n\ntest_tokens_tensor = torch.tensor(test_tokens_ids)\ntest_y_tensor = torch.tensor(test_y, dtype=torch.long)\n\ntrain_masks_tensor = torch.tensor(train_masks,  dtype=torch.long)\ntest_masks_tensor = torch.tensor(test_masks,  dtype=torch.long)\n\nstr(torch.cuda.memory_allocated(device)/1000000 ) + 'M'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert a;; to dataloader to save memory later\ntrain_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\ntrain_sampler = RandomSampler(train_dataset)\ntrain_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n\ntest_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"loss_func = nn.BCEWithLogitsLoss().cuda()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# todo:\n# split data into validation set also?\n# add code for validation evalutation?\n# clean up this notebook a bit.\n# evaluate the classifier\n# ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"token_ids.shape, masks.shape, labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"losses = []\nsteps = []\nstep = 0\nfor epoch_num in range(EPOCHS):\n    bert_clf.train()\n    train_loss = 0\n    for step_num, batch_data in enumerate(train_dataloader): # train for each epoch\n        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n        optimizer.zero_grad() # clear gradients\n        loss =  bert_clf(input_ids=token_ids, attention_mask=masks, labels=labels) # forward pass\n        losses.append(loss.item())\n        loss.backward() # backward pass\n        #import ipdb\n        #ipdb.set_trace()\n\n        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n        \n        optimizer.step() # update params and step using computed gradient\n        \n        clear_output(wait=True)\n        \n        train_loss += loss.item()\n        steps.append(step)\n        step += 1\n        print('Epoch: ', epoch_num + 1)\n        print(\"{0}/{1} loss: {2} \".format(step_num, len(train) / BATCH_SIZE, train_loss / (step_num + 1)))\n        \n    torch.save({\n            'epoch': epoch_num,\n            'model_state_dict': bert_clf.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict(),\n            'loss': loss\n            }, \"../working/news_bert_chpt.pt\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = \"../working/news_bert_statedict.pt\"\ntoch.save(bert_clf.state_dict(), PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH = \"../working/news_bert.pt\"\ntoch.save(bert_clf, PATH)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"bert_clf.eval() # put model in evaluation mode\nbert_predicted = []\nwith torch.no_grad():\n    for step_num, batch_data in enumerate(test_dataloader):\n\n        token_ids, masks = tuple(t.to(device) for t in batch_data)\n        logits_predictions =  bert_clf(input_ids=token_ids, attention_mask=masks) # forward pass\n        \n        logits = logits_predictions.detach().cpu().numpy() #move to cpu\n        \n        bert_predicted += (np.argmax(logits, axis=1)).tolist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(test_y, bert_predicted))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}